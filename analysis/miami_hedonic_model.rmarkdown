---
title: "Predicting Miami Home Prices"
author: "Anna Duan and Bingchu Chen"
date: "10/16/2020"
categories: [R, Housing, Prediction]
format: 
  html:
    toc: true
    code-fold: true
    fontsize: 11pt
execute:
  echo: true
---


## Introduction
Accurate prediction of home sale prices is important right now as the real estate market is seeing record levels of activity due to the pandemic. In this report, we construct a new hedonic model for Zillow's housing market predictions. This task is challenging due to the number of factors which affect the real estate market and the non-linear relationship between many factors and prices. In this model which is built for Miami and Miami Beach, we incorporate local intelligence from open sourced data to adapt it to local housing and development patterns. We use determinants of home prices including internal characteristics, nearby amenities and dis-amenities, and spatial processes such as clustering to estimate home sale prices. Applying this model to a set of 3503 houses, it predicted that home sale prices are highest on the shoreline and in Miami Beach.



```{r setup, include = FALSE}
options(scipen = 999)

knitr::opts_chunk$set(
  echo = TRUE,
  fig.ritna = 3,
  message = FALSE,
  warning = FALSE
)

####load libraries, etc
library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(knitr)
library(gridExtra)
library(ggcorrplot)
library(mapview)
library(osmdata)
library(tidycensus)
library(pander)
library(tigris)

library(conflicted)
conflicts_prefer(dplyr::select)
conflicts_prefer(dplyr::filter)

palette5 <- c("#c8ddfa", "#8cb5ed", "#5890db",   "#2868bd", "#023578")
paletteMap <- c("gray90","gray70","gray50","gray30","gray10")

qBr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                 c(.01,.2,.4,.6,.8), na.rm=T)
  }
}

q5 <- function(variable) {as.factor(ntile(variable, 5))}

#nearest neighbor function
nn_function <- function(measureFrom, measureTo,k) {
  measureFrom_Matrix <- as.matrix(measureFrom)
  measureTo_Matrix <- as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
  output <-
    as.data.frame(nn) %>%
    rownames_to_column(var = "thisPoint") %>%
    gather(points, point_distance, V1:ncol(.)) %>%
    arrange(as.numeric(thisPoint)) %>%
    group_by(thisPoint) %>%
    summarize(pointDistance = mean(point_distance)) %>%
    arrange(as.numeric(thisPoint)) %>%
    dplyr::select(-thisPoint) %>%
    pull()

  return(output)  
}
```

```{r boundaries/mapping layers}
#STUDY AREA
miamiBound <- st_read("/Users/annaduan/Desktop/GitHub/Miami-Home-Sales-Prediction/Raw\ Data/Municipal_Boundary.geojson") %>%
  filter(NAME == "MIAMI BEACH" | NAME == "MIAMI") %>%
  st_make_valid() %>%
  group_by(NAME) %>%
  summarize(geometry = st_union(geometry)) %>%
  erase_water() %>%
  st_transform('ESRI:102658') # NAD 1983 StatePlane Florida East FIPS 0901 Feet

centroids <- miamiBound %>%
  st_centroid()

# background land 
bg <- counties(state = "FL") %>%
  st_transform('ESRI:102658') %>%
  st_crop(st_bbox(st_buffer(miamiBound, 5000))) %>%
  erase_water()

# background roads
rd <- roads(state = "FL", county = "Miami-Dade") %>%
  st_transform('ESRI:102658') %>%
  st_crop(st_bbox(st_buffer(miamiBound, 5000)))

# water
water_rect <- st_as_sfc(st_bbox(st_buffer(miamiBound, 5000)), crs = "ESRI:102658")
```


## Data
For this analysis, we have a dataset of houses and their internal characteristics including number of rooms, living area, and pools. In addition, we gathered open-sourced data from the American Community Survey, Miami-Dade County's Open Data Hub, OpenStreetMaps. This includes  census-tract level demographic information and point and polygon layers of amenities including restaurants and parks (Table 1). It is expected that attributes that contribute to quality of life such as proximity to restaurants, park space, and low commuting times correlate positively with sale prices.


```{r house data}
houses <- st_read("/Users/annaduan/Desktop/GitHub/Miami-Home-Sales-Prediction/Raw\ Data/studentsData.geojson") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658') %>%
  st_centroid() %>%
  mutate(id = as.character(Folio),
         price = as.numeric(SalePrice),
         sale_type = as.factor(saleType),
         city = as.factor(Property.City),
         zip_code = as.factor(substr(Property.Zip, 1, 5)),
         zoning = as.factor(Zoning),
         sqft = AdjustedSqFt,
         lot_size = LotSize,
         beds = Bed,
         baths = Bath,
         stories = Stories,
         units = Units,
         year_built = as.factor(YearBuilt),
         eff_year_built = EffectiveYearBuilt,
         feature_1 = as.factor(XF1),
         feature_2 = as.factor(XF2),
         feature_3 = as.factor(XF3),
         living_sqft = LivingSqFt,
         actual_sqft = ActualSqFt,
         address = Property.Address) %>%
  dplyr::select(id, price, address, sale_type, city, zip_code, zoning, sqft, lot_size, beds, baths, stories, units, year_built, eff_year_built, feature_1, feature_2, feature_3, living_sqft, actual_sqft, toPredict)

ggplot() +
  geom_sf(data = water_rect, fill = "lightblue3", color = "transparent") +
  geom_sf(data = bg, fill = "gray15", color = "transparent") +
  geom_sf(data = miamiBound, fill = "gray20", color = "transparent", size = 0.5) +
  geom_sf(data = rd, color = "gray5", fill = "transparent", linewidth = 0.1) +
  geom_sf(data = parks %>% st_union(), fill = "chartreuse4", color = "transparent", alpha = 0.3) +
  geom_sf(data = houses %>% filter(price > 0 & price < 1800000), aes(color = price/1000), size = 0.5, alpha = 0.5) +
  scale_color_viridis_c(option = "plasma") +
  geom_sf_label(data = centroids, aes(label = str_to_title(NAME)), size = 5, color = "gray20") +
  labs(title = "Distribution of House Prices", caption = "filtered for price < $3 million (95th percentile)", color = "Sale Price\n($1k)") +
  theme_void() + theme(legend.position = c(0.85, 0.2))


#4: AdjustedSqFt
ggplot() +
    geom_sf(data = water_rect, fill = "lightblue3", color = "transparent") +
  geom_sf(data = bg, fill = "gray15", color = "transparent") +
  geom_sf(data = miamiBound, fill = "gray20", color = "transparent", size = 0.5) +
  geom_sf(data = rd, color = "gray5", fill = "transparent", linewidth = 0.1) +
  geom_sf(data = parks %>% st_union(), fill = "chartreuse4", color = "transparent", alpha = 0.3) +
  geom_sf(data = houses %>% filter(actual_sqft < quantile(houses$actual_sqft, 0.95)), aes(color = actual_sqft/1000), size = 0.5, alpha = 0.5) +
  scale_color_viridis_c(option = "magma", direction = 1, name = "Square Feet\n(thousands)") +
    geom_sf_label(data = centroids, aes(label = str_to_title(NAME)), size = 5, color = "gray20") +
  labs(title = "Distribution of Actual Square Feet", subtitle = "Miami and Miami Beach, FL") +
  theme_void() + theme(legend.position = c(0.85, 0.2))
```

```{r house cat vars}
#MAKE CATEGORICAL VARIABLES
 houses <-
  houses %>%
  mutate(beds_bi = ifelse(beds <= 4, "<=4 beds", ">4 beds"),
         bath_bi = ifelse(baths <= 3, "<=3 baths", ">3 baths"),
         sqft_bi = ifelse(sqft <= 2379, "<=2379 sqft", "> 2379 sqft"))

houses <- houses %>%
  mutate(
    Pool = ifelse(
      str_detect(feature_1, "Pool") |
        str_detect(feature_2, "Pool") |
        str_detect(feature_3, "Pool") |
        str_detect(feature_1, "Whirlpool") |
        str_detect(feature_2, "Whirlpool") |
        str_detect(feature_3, "Whirlpool") |
        str_detect(feature_1, "Jacuzzi") |
        str_detect(feature_2, "Jacuzzi") |
        str_detect(feature_3, "Jacuzzi"),
      "1",
      "0"
    ),
    Patio = ifelse(
      str_detect(feature_1, "Patio") |
        str_detect(feature_2, "Patio") |
        str_detect(feature_3, "Patio"),
      "1",
      "0"
    ),
    Fence = ifelse(
      str_detect(feature_1, "Fence") |
        str_detect(feature_2, "Fence") |
        str_detect(feature_3, "Fence"),
      "1",
      "0"
    ),
    Gazebo = ifelse(
      str_detect(feature_1, "Gazebo") |
        str_detect(feature_2, "Gazebo") |
        str_detect(feature_3, "Gazebo"),
      "1",
      "0"
    ),
    Carport = ifelse(
      str_detect(feature_1, "Carport") |
        str_detect(feature_2, "Carport") |
        str_detect(feature_3, "Carport"),
      "1",
      "0"
    ),
    Wall = ifelse(
      str_detect(feature_1, "Wall") |
        str_detect(feature_2, "Wall") |
        str_detect(feature_3, "Wall"),
      "1",
      "0"
    ),
    Dock = ifelse(
      str_detect(feature_1, "Dock") |
        str_detect(feature_2, "Dock") |
        str_detect(feature_3, "Dock"),
      "1",
      "0"
    )
  )

houses %>%
  str() %>%
  pander()
```


## Exploratory Analysis  
### Internal Features  

```{r expl viz}
houses_known <- houses %>% 
  filter(toPredict == 0)

correlation.long <- houses_known %>%
  filter(price < 10000000) %>% # removes about 30 outliers, or <1 %
  st_drop_geometry() %>%
    dplyr::select(price, sqft, lot_size, beds, baths, stories, units, living_sqft, actual_sqft) %>%
    gather(Variable, Value, -price) %>%
  mutate(Value = as.numeric(Value))

correlation.cor <-
  correlation.long %>%
    group_by(Variable) %>%
    summarize(correlation = cor(Value, price, use = "complete.obs"))


ggplot(correlation.long, aes(Value, price)) +
  geom_point(alpha = 0.5, color = "lightblue") +
  geom_text(data = correlation.cor, aes(label = paste("r =", round(correlation, 2))),
            x=-Inf, y=Inf, vjust = 1.5, hjust = -.1) +
  geom_smooth(method = "lm", se = FALSE, color = "gray") +
  facet_wrap(~Variable, ncol = 3, scales = "free") +
  labs(title = "Sale Price as a Function of Features") +
  theme_minimal()


vars <- unique(correlation.long$Variable)
histList <- list()

#map risk factors
for(i in vars){
  histList[[i]] <- 
    ggplot() +
      geom_histogram(data = houses, aes(x = .data[[i]]), stat = "count", fill = "lightblue") +
      labs(title=i) +
      theme_minimal() +
      theme(plot.title = element_text(size=10))
}

do.call(grid.arrange,c(histList, ncol = 3, top = "Continuous Feature Distributions"))
```

```{r categorical features}
houses_cat <- houses_known %>%
  dplyr::select(price, sale_type, city, zoning, beds_bi, bath_bi, sqft_bi, Pool, Patio, Fence, Gazebo, Carport, Wall, Dock, year_built)

vars <- colnames(houses_cat) %>%
  .[! . %in% c("price", "geometry")]
plotList <- list()

#map risk factors
for(i in vars){
  plotList[[i]] <- 
ggplot(houses_cat, aes(x = .data[[i]], y = price/1000)) +
  stat_summary(fun = mean, geom = "col", fill = "lightblue") +
  theme_minimal()
}

do.call(grid.arrange,c(plotList, ncol = 3, top = "Continuous Feature Distributions"))

```


## External Features

```{r external features}
#STUDY AREA OSM (not projected)
miamiBoundOSM <- st_read("/Users/annaduan/Desktop/GitHub/Miami-Home-Sales-Prediction/Raw\ Data/Municipal_Boundary.geojson") %>%
  filter(NAME == "MIAMI BEACH" | NAME == "MIAMI") %>%
  st_make_valid() %>%
  st_union()

#HOUSE DATA OSM (Not projected)
housesOSM <- st_read("/Users/annaduan/Desktop/GitHub/Miami-Home-Sales-Prediction/Raw\ Data/studentsData.geojson")

#CENSUS
census_api_key("d9ebfd04caa0138647fbacd94c657cdecbf705e9", install = TRUE, overwrite = TRUE)

acs <-
  get_acs(geography = "tract", variables = c("B25002_003E", "B25001_001E", "B19013_001E", "B01001A_001E", "B01003_001E", "B07013_002E", "B07013_003E", "B08012_001E", "B25104_001E"), year=2018, state=12, county=086, geometry=T) %>%
  st_transform('ESRI:102658')

#filter for Miami/Miami beach tracts
acs <-
  rbind(
    st_centroid(acs)[miamiBound,] %>%
      st_drop_geometry() %>%
      left_join(acs) %>%
      st_sf() %>%
      mutate(inMiami = "YES"),
    st_centroid(acs)[miamiBound, op = st_disjoint] %>%
      st_drop_geometry() %>%
      left_join(acs) %>%
      st_sf() %>%
      mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES") %>%
  dplyr::select(-inMiami)

#long to wide form
acs <-
  acs %>%
  dplyr::select(-moe, -GEOID) %>%
  spread(variable, estimate) %>%
  dplyr::select(-geometry) %>%
  rename(vacantUnits = B25002_003,
         totalUnits = B25001_001,
         medHHInc = B19013_001,
         white = B01001A_001,
         population = B01003_001,
         ownerOcc = B07013_002,
         renterOcc = B07013_003,
         timeToWork = B08012_001,
         monthhousingcost = B25104_001)

acs["119", "medHHInc"] = 33194   #input value from nearby tract in same neighborhod because NA was messing up MAE

acs %>% na.omit()


#mutate
acs <-
  acs %>%
  mutate(pctVacant = ifelse(totalUnits > 0, vacantUnits / totalUnits, 0),
         pctWhite = ifelse(population > 0, white / population, 0),
         totalOcc = ownerOcc + renterOcc,
         pctRenterOcc = ifelse(totalOcc > 0, renterOcc / totalOcc, 0)) %>%
  dplyr::select(-totalUnits,-vacantUnits,-totalUnits,-population,-white, -ownerOcc, -renterOcc, -totalOcc)


#OSM BBOX (uses the non-projected base)
xmin = st_bbox(miamiBoundOSM)[[1]]
ymin = st_bbox(miamiBoundOSM)[[2]]
xmax = st_bbox(miamiBoundOSM)[[3]]  
ymax = st_bbox(miamiBoundOSM)[[4]]


#FOOD AND BEVERAGE SPOTS
foodBev <- st_read("/Users/annaduan/Desktop/GitHub/Miami-Home-Sales-Prediction/Raw\ Data/food_bev.geojson") %>%
  st_as_sf()


#COASTLINE
Coastline <-opq(bbox = c(xmin, ymin, xmax, ymax)) %>%
  add_osm_feature("natural", "coastline") %>%
  osmdata_sf()

#add to housesOSM and convert to miles, then add to houses
housesOSM <-
  housesOSM %>%  
  mutate(CoastDist=(geosphere::dist2Line(p=st_coordinates(st_centroid(housesOSM)),
                                        line=st_coordinates(Coastline$osm_lines)[,1:2])*0.00062137)[,1])


#PARKS
muniParks <- st_read("https://opendata.arcgis.com/datasets/16fe02a1defa45b28bf14a29fb5f0428_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658') %>%
  dplyr::select(NAME, ADDRESS, CITY, CLASS, Shape__Area)

parks <- 
  # bind_rows(muniParks, countyParks) %>%
  muniParks %>%
  filter(CITY == "Miami" | CITY == "Miami Beach") %>%
  mutate(counter = 1)



#SCHOOL DISTRICT
schoolDist <- st_read("https://opendata.arcgis.com/datasets/bc16a5ebcdcd4f3e83b55c5d697a0317_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658') %>%
  dplyr::select(ID)



#PUBLIC SCHOOL CATCHMENT/ATTENDANCE ZONES
#elementary
elementary <- st_read("https://opendata.arcgis.com/datasets/19f5d8dcd9714e6fbd9043ac7a50c6f6_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')

elementary <- rbind(
  st_centroid(elementary)[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(elementary) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  st_centroid(elementary)[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(elementary) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES") %>%
  dplyr::select(NAME)

#middle
middle <- st_read("https://opendata.arcgis.com/datasets/dd2719ff6105463187197165a9c8dd5c_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')

middle <- rbind(
  st_centroid(middle)[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(middle) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  st_centroid(middle)[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(middle) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES") %>%
  dplyr::select(NAME)

#high
high <- st_read("https://opendata.arcgis.com/datasets/9004dbf5f7f645d493bfb6b875a43dc1_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')

high <- rbind(
  st_centroid(high)[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(high) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  st_centroid(high)[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(high) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES") %>%
  dplyr::select(NAME)


#PUBLIC TRANSPORTATION
#bus
bus <- st_read("https://opendata.arcgis.com/datasets/021adadcf6854f59852ff4652ad90c11_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant")  %>%
  st_transform('ESRI:102658')

bus <- rbind(
  bus[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(bus) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  bus[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(bus) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES")

#metro mover
metromover <- st_read("https://opendata.arcgis.com/datasets/aec76104165c4e879b9b0203fa436dab_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')

metromover <- rbind(
  metromover[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(metromover) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  metromover[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(metromover) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES")

#metro rail
metrorail <- st_read("https://opendata.arcgis.com/datasets/ee3e2c45427e4c85b751d8ad57dd7b16_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')

metrorail <- rbind(
  metrorail[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(metrorail) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  metrorail[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(metrorail) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES")


#CULTURE SPOTS
culture <- st_read("https://opendata.arcgis.com/datasets/70c48f0eb067448c8a787cfa1c1c3bb9_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')

culture <- rbind(
  culture[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(culture) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  culture[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(culture) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES")


#COMMERCIAL PROPERTIES
#read, project
commercial <- st_read("https://opendata.arcgis.com/datasets/fb8303c577c24ea386a91be7329842be_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')

#filter
commercial <- rbind(
  commercial[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(commercial) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  commercial[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(commercial) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES")


#FLOOD RISK ZONES
floodRisk <- st_read("https://opendata.arcgis.com/datasets/ef3bdd041b2e424695eb4dfe965966c4_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')


#filter
 floodRisk <-
   rbind(
  st_centroid(floodRisk)[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(floodRisk) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  st_centroid(floodRisk)[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(floodRisk) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES") %>%
   dplyr::select(-inMiami, -ELEV) %>%
   dplyr::rename(FloodZone = FZONE, FloodHazard = ZONESUBTY)


 floodInsure <- st_read("https://opendata.arcgis.com/datasets/f589473ddada46e78d437aaf09205b04_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')

 floodInsure <-
   rbind(
  st_centroid(floodInsure)[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(floodInsure) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  st_centroid(floodInsure)[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(floodInsure) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES") %>%
   mutate(floodInsureType = PANELID)


#CONTAMINATED SITES
contaminated <- st_read("https://opendata.arcgis.com/datasets/43750f842b1e451aa0347a2ca34a61d7_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')
 

contaminated <-
   rbind(
  st_centroid(contaminated)[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(contaminated) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  st_centroid(contaminated)[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(contaminated) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES")
```


### Feature Engineering

```{r Wrangle Data, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
#CONTAMINATION BUFFER
 contamBuffer <- contaminated %>%
   st_buffer(800) %>%
   st_union() %>%
   st_as_sf() %>%
   mutate(contam = 1)
 houses$contaminated <- houses %>%
   st_join(contamBuffer) %>%
   mutate(contam = ifelse(is.na(contam), 0, 1)) %>%
   pull(contam)


#NEAREST NEIGHBOR (some are used for testing, to determine feature buffer distances)
 st_c <- st_coordinates
 houses <-
   houses %>%
   mutate(
     #commercial properties NN
     commNN1 = nn_function(st_c(st_centroid(houses)), st_c(st_centroid(commercial)), 1),
     commNN5 = nn_function(st_c(st_centroid(houses)), st_c(st_centroid(commercial)), 5),
     #metro mover stations
     metroMNN1 = nn_function(st_c(st_centroid(houses)), st_c(metromover), 1),
     metroMNN5 = nn_function(st_c(st_centroid(houses)), st_c(metromover), 5),
     #metro rail stations
     metroRNN1 = nn_function(st_c(st_centroid(houses)), st_c(metrorail), 1),
     metroRNN5 = nn_function(st_c(st_centroid(houses)), st_c(metrorail), 5),
     #food/drinks
     foodBevNN1 = nn_function(st_c(st_centroid(houses)), st_c(foodBev), 1),
     foodBevNN5 = nn_function(st_c(st_centroid(houses)), st_c(foodBev), 5)
     ) 


#COMMERCIAL BUFFER
 commercial <- commercial %>%
   mutate(counter = 1) %>%
   dplyr::select(counter)
 
 #count properties within each buffer
houses$commercialProperties <-
   st_buffer(houses, 846) %>%
   aggregate(commercial, ., sum) %>%
   st_drop_geometry() %>%
  mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)



#FOOD AND BEV BUFFER
 foodBev <- foodBev %>%
   mutate(counter = 1) %>%
   dplyr::select(counter)
 
 #count parks within each buffer
houses$foodEstablishments <-
   st_buffer(houses, 2774) %>%
   aggregate(foodBev, ., sum) %>%
   st_drop_geometry() %>%
   mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)



#CULTURE BUFFER
 culture <- culture %>%
   mutate(counter = 1) %>%
   dplyr::select(counter)
 #count culture within each buffer
houses$cultureSpots <-
   st_buffer(houses, 774) %>%
   aggregate(culture, ., sum) %>%
   st_drop_geometry() %>%
   mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)



#METRORAIL BUFFER
 metrorail <- metrorail %>%
   mutate(counter = 1) %>%
   dplyr::select(counter)
 #count stops within each buffer
houses$metrorailStops <-
   st_buffer(houses, 12076.7) %>%
   aggregate(metrorail, ., sum) %>%
   st_drop_geometry() %>%
  mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)



#METROMOVER BUFFER
 metromover <- metromover %>%
   mutate(counter = 1) %>%
   dplyr::select(counter)
 #count metroM stops within each buffer
houses$metromoverStops <-
   st_buffer(houses, 18845) %>%
   aggregate(metromover, ., sum) %>%
   st_drop_geometry() %>%
   mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)


#BUS BUFFER
 bus <- bus %>%
   mutate(counter = 1) %>%
   dplyr::select(counter)
 #count bus within each buffer
houses$busStops <-
   st_buffer(houses, 775) %>%
   aggregate(bus, ., sum) %>%
   st_drop_geometry() %>%
  mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)

 #PARKS BUFFER + AREA CALCULATION (using 1600ft buffer distance because the mean NN1 = 1600)
 #get centroids
 parkCentroids <- parks %>%
   st_centroid(parks) %>%    #get centroids of park layer
  dplyr::select(counter)
 
 #count parks within each buffer
houses$parkCount <-
   st_buffer(houses, 1600) %>%
   aggregate(parkCentroids, ., sum) %>%
   st_drop_geometry() %>%
   mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)

#make buffer for each house
parkBuffer <- st_buffer(houses, 1600) %>%
  dplyr::select(address) %>%
  st_as_sf()

#calculate area of park space in each buffer
bufferedParks <- st_intersection(parkBuffer, parks) %>%
  group_by(address) %>%
  summarise() %>%
  mutate(parkArea = units::drop_units(st_area(.))) %>%
  st_drop_geometry()

#add park area back to houses file
houses <-
  left_join(houses, bufferedParks)

#SCHOOL CATCHMENT CATEGORIES
 houses <-
   st_join(houses, elementary) %>%
   rename(elemCatch = 'NAME')

  houses <-
   st_join(houses, middle) %>%
   rename(middleCatch = 'NAME')

   houses <-
   st_join(houses, high) %>%
   rename(highCatch = 'NAME')
   

 #SCHOOL DISTRICT CATEGORIES
 houses <-
   st_join(houses, schoolDist) %>%
   rename(schoolDist = ID)

 #FLOOD INSURANCE CATEGORIES
floodInsure <- floodInsure %>%
  dplyr::select(floodInsureType)

houses <- houses %>%
  st_join(., floodInsure) %>%
  mutate(floodInsureType = ifelse(is.na(floodInsureType), "other", floodInsureType))

#ADD ACS DATA
houses <-
  st_join(houses, acs)

#School dist
 houses <-
   houses %>%
  mutate(elemCatch = ifelse(is.na(elemCatch), "other", elemCatch),
         middleCatch = ifelse(is.na(middleCatch), "other", middleCatch),
         highCatch = ifelse(is.na(highCatch), "other", highCatch))

#park
 houses <- houses %>%
     mutate(parkArea = ifelse(is.na(parkArea), 0, parkArea))

 #acs - here I found the location of NA values and gave these houses the ACS values of houses nearby/in same neighborhood
houses["3487", "NAME"] = houses["1099", "NAME"]
houses["3487","timeToWork"] =  houses["1099","timeToWork"]
houses["3487", "medHHInc"] = houses["1099", "medHHInc"]
houses["3487", "monthhousingcost"] = houses["1099", "monthhousingcost"]
houses["3487", "pctVacant"] = houses["1099", "pctVacant"]
houses["3487", "pctWhite"] = houses["1099", "pctWhite"]
houses["3487", "pctRenterOcc"] = houses["1099", "pctRenterOcc"]


houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "NAME"] = houses["3458", "NAME"]
houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "timeToWork"] = houses["3458", "timeToWork"]
houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "medHHInc"] = houses["3458", "medHHInc"]
houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "monthhousingcost"] = houses["3458", "monthhousingcost"]
houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "pctVacant"] = houses["3458", "pctVacant"]
houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "pctWhite"] = houses["3458", "pctWhite"]
houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "pctRenterOcc"] = houses["3458", "pctRenterOcc"]

houses["441", c("NAME","timeToWork","medHHInc","monthhousingcost","pctVacant","pctWhite","pctRenterOcc")] = houses["2090", c("NAME","timeToWork","medHHInc","monthhousingcost","pctVacant","pctWhite","pctRenterOcc")]


houses <-
  houses %>%
  mutate(distWater = housesOSM$CoastDist)
```


### Correlations 
First, we use a correlation matrix to get an idea of different external variables' correlations with sale price. 

	
Surprisingly, the share of a census tract that is vacant has a positive correlation with sale price (Figure 2.3). This is unexpected because typically, vacant houses correlate with neighborhood disorder and unattractiveness. However, it is possible that these vacancies are the result of new construction and therefore do not make the area less attractive.


```{r Figure 1: Correlation Matrix, message=FALSE, warning=FALSE}

corrPlotVars <- houses %>%
  dplyr::select(price, commNN1, commNN5, metroMNN1, metroMNN5, metroRNN1, metroRNN5, foodBevNN1, foodBevNN5, commercialProperties, foodEstablishments, cultureSpots, metrorailStops, metromoverStops, busStops, parkCount, parkArea, timeToWork, medHHInc, monthhousingcost, pctVacant, pctWhite, pctRenterOcc, distWater) %>% 
  na.omit() %>%
  st_drop_geometry()

ggcorrplot(
  round(cor(corrPlotVars), 1),
  p.mat = cor_pmat(corrPlotVars),
  colors = c("cyan4", "white", "magenta"),
  type="lower",
  lab = TRUE,
  lab_size = 2,
  lab_col = "white",
  insig = "blank") +  
    labs(title = "Sale Price Correlation with Numeric Variables", x = "", y = "", color = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 270, hjust = 0),
        legend.position = "bottom")


correlation.long <- houses %>%
  filter(price < 10000000 & price > 0) %>% # removes about 30 outliers, or <1 %
  st_drop_geometry() %>%
    dplyr::select(price, commNN1, commNN5, metroMNN1, metroMNN5, metroRNN1, metroRNN5, foodBevNN1, foodBevNN5, commercialProperties, foodEstablishments, cultureSpots, metrorailStops, metromoverStops, busStops, parkCount, parkArea, timeToWork, medHHInc, monthhousingcost, pctVacant, pctWhite, pctRenterOcc, distWater) %>%
    gather(Variable, Value, -price) %>%
  mutate(Value = as.numeric(Value))

correlation.cor <-
  correlation.long %>%
    group_by(Variable) %>%
    summarize(correlation = cor(Value, price, use = "complete.obs")) %>%
  arrange(desc(correlation))

pander(correlation.cor %>% filter(abs(correlation) > 0.3), caption = "Correlation of Variables with Sale Price")
```


### Median Household Income
Three variables that we will explore further are median household income (cor = 0.53), vacancy rate (cor = 0.49), and distance from commercial properties (cor = 0.38). 


```{r medhhinc, message=FALSE, warning=FALSE}
houses_known <- houses %>%    
  filter(.,toPredict == 0)
houses_unknown <- houses %>%
  filter(.,toPredict ==1)

ggplot() +
    geom_sf(data = water_rect, fill = "lightblue3", color = "transparent") +
  geom_sf(data = bg, fill = "gray15", color = "transparent") +
  geom_sf(data = miamiBound, fill = "gray20", color = "transparent", size = 0.5) +
  geom_sf(data = rd, color = "gray5", fill = "transparent", linewidth = 0.1) +
  geom_sf(data = parks %>% st_union(), fill = "chartreuse4", color = "transparent", alpha = 0.3) +
  geom_sf(data = houses, aes(color = medHHInc/1000), size = 0.5) +
  scale_color_viridis_c(option = "magma", name = "Dollars\n(thousands)") +
  labs(title = "Median Household Income") +
  theme_void() + theme(legend.position = c(0.85, 0.2))
```




### Vacancy Rate


```{r Fig 3: MAP Sale Price, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, results='hide'}
#map of your dependent variable (sale price)
  #water is just for mapping visuals
ggplot() +
  geom_sf(data = water_rect, fill = "lightblue3", color = "transparent") +
  geom_sf(data = bg, fill = "gray15", color = "transparent") +
  geom_sf(data = miamiBound, fill = "gray20", color = "transparent", size = 0.5) +
  geom_sf(data = rd, color = "gray5", fill = "transparent", linewidth = 0.1) +
  geom_sf(data = parks %>% st_union(), fill = "chartreuse4", color = "transparent", alpha = 0.3) +
  geom_sf(data = housesKnown %>% filter(price < 1800000), aes(colour = price/1000)) +
  scale_color_viridis_c(direction = 1, option = "C") +
  geom_sf_label(data = centroids, aes(label = str_to_title(NAME)), size = 5, color = "gray20") +
  labs(title = "Home Sale Price", color = "Price ($1k)") +
  theme_void()
```


### Distance from Commercial Properties


It can be expected that distance to water is negatively correlated to sale price in a city known for its beaches. Figure 4.1 shows that distance to the shoreline closely matches sale price. 

However, it's possible that sale price appears to be strongly correlated with distance to water because of other factors. Downtown Miami is located near the beach, and the heightened development around the beaches and downtown may give the area other attractive features.

Two of these features are green space and dining venues. Figure 4.2 shows that houses in Miami Beach have the most green space nearby. Interestingly, only some of the high sale price houses have high park area, mostly in Miami Beach. This may be due to crime incidents in park areas. In Figure 4.3, we see similar patterns, with the highest values in the houses in Miami Beach and the north part of Miami's shoreline. This suggests that distance to the shoreline is only part of the story.

Outside of amenities, we expect that sale price correlates with median household income of census tracts and the adjusted square footage of houses. In Figures 4.4 and 4.5, we see this confirmed: much like sale price, the highest values for income and square footage are along the shoreline. 

### Distance from shore

```{r dist from shore}

ggplot() +
    geom_sf(data = water_rect, fill = "lightblue3", color = "transparent") +
  geom_sf(data = bg, fill = "gray15", color = "transparent") +
  geom_sf(data = miamiBound, fill = "gray20", color = "transparent", size = 0.5) +
  geom_sf(data = rd, color = "gray5", fill = "transparent", linewidth = 0.1) +
  geom_sf(data = parks %>% st_union(), fill = "chartreuse4", color = "transparent", alpha = 0.3) +
  geom_sf(data = houses, aes(colour = distWater)) +
  scale_color_viridis_c(option = "magma", name = "Miles") +
  labs(title = "Distance to Shore") +
  theme_void()
```


### K-nearest sale prices

```{r nearest sale prices}

houses_known <-  houses %>% filter(toPredict == 0)

nb_list <- knn2nb(knearneigh(houses_known, 6))

spatial_weights <- nb2listw(nb_list, style="W")

houses_known <- houses_known %>% 
  mutate(price_nearest_5 = lag.listw(spatial_weights, price))

ggplot() +
  geom_sf(data = water_rect, fill = "lightblue3", color = "transparent") +
  geom_sf(data = bg, fill = "gray15", color = "transparent") +
  geom_sf(data = miamiBound, fill = "gray20", color = "transparent", size = 0.5) +
  geom_sf(data = rd, color = "gray5", fill = "transparent", linewidth = 0.1) +
  geom_sf(data = parks %>% st_union(), fill = "chartreuse4", color = "transparent", alpha = 0.3) +
  geom_sf(data = houses_known %>% filter(price_nearest_5 < 3098907), aes(colour = price_nearest_5/1000)) +
  scale_color_viridis_c(direction = 1, option = "magma") +
  geom_sf_label(data = centroids, aes(label = str_to_title(NAME)), size = 5, color = "gray20") +
  labs(title = "K-nearest sale prices (k=6)", color = "Price ($1k)") +
  theme_void()
```



## Methods 
Now knowing that attractive amenities, high income households, large houses, and high sale prices cluster around the shoreline area, we can begin to test features to put in our regression model. We ranked the correlation of each feature with sale price, then added them in order until each addition feature no longer increased the model's predictive power for sale price. The regression model that we chose includes house internal features, census tract variables, transportation availability, park area, and flood risk.



```{r Designing Regression, message=FALSE, warning=FALSE, results='hide'}

 #0.72 R2
  reg2 <- lm(SalePrice ~ ., data = st_drop_geometry(housesKnown) %>%
             dplyr::select(price, actual_sqft, lot_size, zoning, Stories.cat, Bath.cat, Pool, medHHInc, Dock, Bed.cat, middleCatch, age, pctVacant, pctRenterOcc, monthhousingcost, Patio, foodEstablishments, timeToWork, metromoverStops, metrorailStops, parkArea, floodInsureType))
```



Next, to test this model, we used houses with observed sale prices to create a training set for training our model, and a test set for testing it. 



```{r Train the regression, message=FALSE, warning=FALSE, results='hide'}
#read FL neighborhoods
#nhoods_fl <- aoi_boundary_HARV <- st_read("E:/Upenn/CPLN508/miami/zillow_nghbrhd_feb17/zillow_nghbrhd_feb17.shp")
nhoods_fl <- aoi_boundary_HARV <- st_read("/Users/annaduan/Desktop/GitHub/Miami-Home-Sales-Prediction/Raw\ Data/zillow_nghbrhd_feb17/zillow_nghbrhd_feb17.shp")
nhoods_mb <- subset(nhoods_fl, CITY == "MIAMI BEACH")%>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')
nhoods_m <- subset(nhoods_fl, CITY == "MIAMI")%>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')

#Join neighborhoods
nhoods <- rbind(nhoods_mb, nhoods_m)
nhoods <- nhoods %>%
  dplyr::select(NAME) %>%
  rename(neighborhood = NAME)
housesKnown <- housesKnown %>% st_join(., nhoods, join = st_within)
houses <- houses %>% st_join(., nhoods, join = st_within)
housesUnknown <- housesUnknown %>% st_join(., nhoods, join = st_within)

#Separate test/train sets

inTrain <- createDataPartition(
              y = paste(housesKnown$Zoning, housesKnown$floodInsureType, housesKnown$neighborhood),
              p = .60, list = FALSE)
miami.training <- housesKnown[inTrain,]
miami.test <- housesKnown[-inTrain,]  


#Training regression 
reg.training <- 
  lm(SalePrice ~ ., data = st_drop_geometry(miami.training) %>% 
                             dplyr::select(SalePrice, ActualSqFt, LotSize, Zoning, Stories.cat, Bath.cat, Pool, medHHInc, Dock, Bed.cat, middleCatch, age, pctVacant, pctRenterOcc, monthhousingcost, Patio, foodEstablishments, timeToWork, metromoverStops, metrorailStops, parkArea, floodInsureType))
```



## Results
The results of the regression model on the training set are presented in Table 2. Overall, the R-squared value of 0.9 means our variables explain most of the variation in sale price. The low p values also indicate a high level of confidence.



```{r Table 2: TABLE Training Set lm, message=FALSE, warning=FALSE, include=TRUE}
#polished table of  (training set) lm summary results (coefficients, R2 etc)
stargazer(reg.training, type="text", digits=1, title="Table 2: LM of Training Data", out = "Training LM.txt")
```



Next, we test the model on our test set to see its effectiveness on new data. Overall, it is relatively accurate: the average percentage error is 7.1%. However, the mean absolute error is $ 351,281, which is concerning as the average price in the test set is $ 689,606. This may be because our model is less accurate for expensive houses, as the absolute errors for them is higher at a given percentage error. Indeed, in Figure 5.1 and 5.2, absolute error is higher for homes with high observed prices, but  percent error is consistently low, at less than 10%.



```{r Figure 5.1-5.2 Test the regression, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, results='hide'}
#Test regression on miami.test
miami.test <-
  miami.test %>%
  mutate(Regression = "Baseline Regression",
         SalePrice.Predict = predict(reg.training, miami.test), #751571.5
         SalePrice.Error = SalePrice.Predict - SalePrice, #60608.35
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice), #363363
         SalePrice.APE = SalePrice.AbsError / SalePrice) %>% #0.05589877  #corrected 
  filter(SalePrice < 5000000) 

#Mean error and APE 432255.1   0.3407089   443611.7 0.1643853 467809 -0.0241117
mean(miami.test$SalePrice.AbsError, na.rm = T)#[1] 351280.6
mean(miami.test$SalePrice.APE, na.rm = T)#[1] 0.7101703
mean(miami.test$SalePrice.Predict, na.rm = T)#[1] 770165.4

ggplot(data = miami.test) +
  geom_point(aes(x = SalePrice, y = SalePrice.AbsError)) +
  labs(title = "Figure 5.1: Observed Sale Price and Absolute Error") +
  theme_minimal()

ggplot(data = miami.test) +
  geom_point(aes(x = SalePrice, y = SalePrice.APE)) +
  labs(title = "Figure 5.2: Observed Sale Price and Absolute Percent Error") +
  theme_minimal()
                                     
```



### Is our model generalizable?
In addition to accuracy, generalizability is important for our model to be effective. To do this, we run a K-folds test to test the model on different segments of our test set.

We see in Table 3 that Fold75, one of the 100 partitioned segments of our test data, has an adjusted R-squared of 0.78 and a mean average error of $350,348.9. While the R^2 is high, the error is more than half of the average predicted price across all folds. These results are similar to the results for our training, which suggests strong generalizability across groups.



```{r Table 3: TABLE of MAE and MAPE for single k- fold test set, message=FALSE, warning=FALSE, include=TRUE}

#K Folds test
fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(SalePrice ~ ., data = st_drop_geometry(housesKnown) %>% 
                                dplyr::select(SalePrice, ActualSqFt, LotSize, Zoning, Stories.cat, Bath.cat, Pool, medHHInc, Dock, Bed.cat, middleCatch, age, pctVacant, pctRenterOcc, monthhousingcost, Patio, foodEstablishments, timeToWork, metromoverStops, metrorailStops, parkArea, floodInsureType), 
     method = "lm", trControl = fitControl, na.action = na.pass)


#k-fold function online
kfold.MLR = function(fit,k=10,data=fit$model) {    
  sum.sqerr = rep(0,k)
  sum.abserr = rep(0,k)
  sum.pererr = rep(0,k)
  y = fit$model[,1]
  x = fit$model[,-1]
  n = nrow(data)
  folds = sample(1:k,nrow(data),replace=T)
  for (i in 1:k) {
    fit2 <- lm(formula(fit),data=data[folds!=i,])
    ypred = predict(fit2,newdata=data[folds==i,])
    sum.sqerr[i] = sum((y[folds==i]-ypred)^2)
    sum.abserr[i] = sum(abs(y[folds==i]-ypred))
    sum.pererr[i] = sum(abs(y[folds==i]-ypred)/y[folds==i])
  }
  cv = return(data.frame(RMSEP=sqrt(sum(sum.sqerr)/n),
                         MAE=sum(sum.abserr)/n,
             MAPE=(sum(sum.pererr)/n)*100))
}

#ADDED TODAY
reg.cv.known <-
  housesKnown %>%
  mutate(Regression = "Baseline Regression",
         SalePrice.Predict = predict(reg.cv, housesKnown), #751571.5
         SalePrice.Error = SalePrice.Predict - SalePrice, #60608.35
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice), #363363
         SalePrice.APE = SalePrice.AbsError / SalePrice) %>% #0.05589877  #corrected 
  filter(SalePrice < 5000000) 
fold75 <- reg.cv$control$indexOut$Resample075

reg75 <- reg.cv.known[fold75,c("SalePrice", "SalePrice.Predict")]
reg75.test <-
  reg75 %>%
  mutate(SalePrice.Error = SalePrice.Predict - SalePrice, 
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice), 
         SalePrice.APE = SalePrice.AbsError / SalePrice) %>% 
  filter(SalePrice < 5000000) 
reg.cv.rs.min <- reg.cv$resample[75,]
reg.cv.rs.min$MAPE <- mean(reg75.test$SalePrice.APE)

round_df <- function(x, digits) {
    # round all numeric variables
    # x: data frame 
    # digits: number of digits to round
    numeric_columns <- sapply(x, mode) == 'numeric'
    x[numeric_columns] <-  round(x[numeric_columns], digits)
    x
}
reg.cv.rs.min <- round_df(reg.cv.rs.min, 2)



reg.cv.rs.min <- reg.cv$resample[75,]


reg.cv.rs.min %>%                     
  gather(Variable, Value) %>%
  #filter(Variable == "MAE" | Variable == "RMSE") %>%
  group_by(Variable) %>%
    spread(Variable, Value) %>%
    pander(caption = "Table 3: Regression Results of One Test Set") 
```



For more context, Figure 6.1 shows that most test sets have errors around $400,000, but that a few outliers skew the average error upward. If the model is generalizable, it should have a clustered distribution of errors. As this is not the case, it appears that the model is not actually predicting consistently across groups of houses.



```{r Fig 6.1: HISTOGRAM results of cross-validation tests, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, results='hide'}
ggplot(reg.cv$resample, aes(x=MAE)) +
  geom_histogram() +
  labs(title = "Figure 6.1: Mean Average Error in Cross Validation Tests") +
  theme_minimal()

```



### Do errors cluster spatially?
One way to investigate the reason for the model's inconsistency is to look at how it treats houses across space. 



```{r Spatial Lag, message=FALSE, warning=FALSE, results='hide'}
library(knitr)
library(kableExtra)
library(scales)

coords <- st_coordinates(housesKnown)
neighborList <- knn2nb(knearneigh(coords, 5)) #5 nearest neighborhoods

spatialWeights <- nb2listw(neighborList, style="W") #not sure what is W here
housesKnown$lagPrice <- lag.listw(spatialWeights, housesKnown$SalePrice)
#plot(housesKnown$SalePrice, housesKnown$lagPrice)

coords.test <-  st_coordinates(miami.test)
neighborList.test <- knn2nb(knearneigh(coords.test, 5))
spatialWeights.test <- nb2listw(neighborList.test, style="W")

```



In Figure 7.1, we can see that residuals are evenly distributed spatially. This suggests that the low generalizability of the data is not due to spatial processes, but other factors. 



```{r Figure 7.1: MAP of residuals for test set, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, results='hide'}

#10.1 Map of test set residuals
library(modelr)

miami.test$resid <- 
  miami.test %>%
  as_data_frame() %>%
  add_residuals(., reg.training, var = "resid") %>%
  dplyr::select(resid, Folio) %>%
  pull(resid)

ggplot() +
geom_sf(data = acs, fill = "gray90", colour = "white") +
    geom_sf(data = water, fill = "light blue", colour = "light blue") +
  geom_sf(data = miami.test, aes(colour = q5(resid))) +
  scale_colour_manual(values = palette5) +
 labs(title = "Figure 7.1: Test Set Residual Errors", subtitle = "Miami and Miami Beach, FL") +
  theme_void()

```



To see more clearly the effect of spatial processes on our errors, we can look at spatial lags, or the clustering of prices and errors. In Figure 8.1, we can see that neighboring houses' price estimate errors do not increase in the same way with sale price. This again tells us that most of our errors are not spatial in nature.



```{r Figure 8.1: Spatial lag, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, results='hide'}

miami.test %>%                
  mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)) %>%  
  ggplot(aes(lagPriceError, SalePrice)) +
  geom_point() +
  stat_smooth(aes(lagPriceError, SalePrice), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800")+
  labs(title = "Figure 8.1: Spatial Lag of Price Errors") +
  theme_minimal() + theme(plot.title = element_text(size = 18, colour = "black")) 

```



Finally, we can use a Moran's I test to gain further insight into the spatial autocorrelation of our model errors. If our model errors are not influenced by spatial processes, we should see a Moran's I of 0. Our Moran's I value is less than 0.2, which confirms that we have very minimal spatial clustering of errors.



```{r Figure 9.1: Morans I, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, results='hide'}

moranTest <- moran.mc(miami.test$SalePrice.Error,
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Figure 9.1: Observed and Permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  theme_minimal()
```



### Accounting for neighborhood variance
We now add neighborhood as a feature in our model to account for the differences in sale price across neighborhoods. Table 4 tells us that when we account for neighborhood effects, we actually slightly increase the absolute error but we decrease the absolute percentage error. This may mean that the neighborhood model works best for lower priced homes, and that it increased the error in expensive ones. It also tells us that our Baseline model already accounted for spatial disparities through our use of open data features.



```{r Table 4: Neighborhood Regression, message=FALSE, warning=FALSE, include=TRUE}

#Make new neighborhood regression
reg.nhood <- lm(SalePrice ~ ., data = as.data.frame(miami.training) %>% 
                                 dplyr::select(neighborhood, SalePrice, ActualSqFt, LotSize, Zoning, Stories.cat, Bath.cat, Pool, medHHInc, Dock, Bed.cat, middleCatch, age, pctVacant, pctRenterOcc, monthhousingcost, Patio, foodEstablishments, timeToWork, metromoverStops, metrorailStops, parkArea, floodInsureType))

#Outcomes
miami.test.nhood <-
  miami.test %>%
  mutate(Regression = "Neighborhood Effects",
         SalePrice.Predict = predict(reg.nhood, miami.test), #613237.3
         SalePrice.Error = SalePrice - SalePrice.Predict, # -108442.4      
         SalePrice.AbsError = abs(SalePrice - SalePrice.Predict), # 491238.7
         SalePrice.APE = (abs(SalePrice - SalePrice.Predict)) / SalePrice)%>% #0.7109973
  filter(SalePrice < 5000000)

#Check accuracy
bothRegressions <-
  rbind(
    dplyr::select(miami.test, starts_with("SalePrice"), Regression, neighborhood) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)),
    dplyr::select(miami.test.nhood, starts_with("SalePrice"), Regression, neighborhood) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)))   

st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -neighborhood) %>%
  filter(Variable == "SalePrice.AbsError" | Variable == "SalePrice.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    kable(caption = "Table 4: Neighborhood Effect on Error")

```



In Figure 10.1, we can see the effect of neighborhood model on the accuracy of our predictions. As suspected, the neighborhood model fits the data only marginally better. For some of the higher priced homes, however, it appears that adding neighborhood as a feature causes an overestimation of price. This may be due to variations within neighborhoods.



```{r Fig 10.1:PLOT predicted prices as a function of observed prices, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, results='hide'}

bothRegressions %>%
  dplyr::select(SalePrice.Predict, SalePrice, Regression) %>%
    ggplot(aes(SalePrice, SalePrice.Predict)) +
  geom_point() +
  stat_smooth(aes(SalePrice, SalePrice),
             method = "lm", se = FALSE, size = 1, colour="#FA7800") +
  stat_smooth(aes(SalePrice.Predict, SalePrice),
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  facet_wrap(~Regression) +
  labs(title="Figure 10.1: Predicted Sale Price and Observed Price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  theme_minimal() + theme(plot.title = element_text(size = 18, colour = "black"))

```



Finally, Figure 11.1 shows the prices that we predicted for the set of 3503 Miami area homes. As we saw with the known home sale prices, our predicted prices are highest close to the shoreline and on Miami Beach. 



```{r Figure 11.1: map of predicted values, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, results='hide'}
#876 rows
#filter by toPredict = 1
housesPredictions <-                   
  houses %>%
  mutate(prediction = predict(reg.nhood, houses),
         team_name = 'Panda')

predictions <- housesPredictions[,c("Folio", "prediction", "team_name", "toPredict")] %>%
  st_drop_geometry() %>%
  filter(toPredict == 1) %>%
  dplyr::select(-toPredict)

# write.csv(predictions, "PANDA.csv") #"The column names MUST to be "prediction", "Folio", and "team_name"" - piazza

#Map values
ggplot() +
  geom_sf(data = acs, fill = "gray90", colour = "white") +
    geom_sf(data = water, fill = "light blue", colour = "light blue") +
  geom_sf(data = housesPredictions, aes(colour = q5(prediction))) +
 scale_colour_manual(values = palette5) +
 labs(title = "Figure 11.1: Predicted Sale Price Values", subtitle = "Miami and Miami Beach, FL") +
 # facet_wrap(~toPredict) +
  theme_void()
```



In Figure 12.1, we can see the spatial distribution of our errors. Overall, it appears that mean average percentage error is lower in Miami Beach, where sale prices are higher. It is highest in the center of Miami, and relatively low on the Miami shoreline. This suggests that our errors were smaller in neighborhoods with higher observed sale prices.



```{r Figure 12.1: Test set predictions, MAPE by neighborhood, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, results='hide'}
#Using the test set predictions, provide a map of mean absolute percentage error (MAPE) by neighborhood
names(bothRegressions)[names(bothRegressions) == "neighborhood"] <- "neighborhood"
st_drop_geometry(bothRegressions) %>%
  group_by(Regression, neighborhood) %>%
  summarise(mean.MAPE = mean(SalePrice.APE, na.rm = T)) %>%
  ungroup() %>%
  left_join(nhoods) %>%
    st_as_sf() %>%
   ggplot() +
    geom_sf(data = water, fill = "light blue", colour = "light blue") +
      geom_sf(colour = "gray", aes(fill = q5(mean.MAPE))) +
      scale_fill_manual(values = paletteMap) +
  labs(title = "Figure 12.1: Mean Average Percentage Error by Neighborhood") +
      theme_void()

```



Figure 13.1 confirms this trend. We can see clearly that with one exception, neighborhoods with lower mean prices have higher mean average percentage errors. 



```{r Figure 13.1: Scatterplot of MAPE by neighborhood as a function of mean price by neighborhood, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, results='hide'}

scatter_hood <-
    miami.test.nhood %>%
    group_by(neighborhood) %>%
    dplyr::select(neighborhood, SalePrice.APE, SalePrice.Predict)

mean_sca_hd <-
  scatter_hood %>%
  group_by(neighborhood) %>%
  summarise_at(vars("SalePrice.APE", "SalePrice.Predict"), mean)

plot(mean_sca_hd$SalePrice.Predict, mean_sca_hd$SalePrice.APE, main="Figure 13.1: MAPE by Neighborhood and Mean Price by Neighborhood", xlab="Mean Price by Neighborhood", ylab="MAPE by neighborhood") +
  theme_minimal()

#https://r-graphics.org/recipe-scatter-labels or we can use the method below:

#scatter_mae_mean <- ggplot(mean_sca_hd, aes(x = SalePrice.Predict, y = SalePrice.APE)) +
#    geom_point() +
#  theme_minimal()

#scatter_mae_mean +    #AD: this is cool but we can't really see which dot is which neighborhood
#  annotate("text", x = 820000, y = -11.7, label = "UPPER EASTSIDE") +
#  annotate("text", x = 330000, y = -6.5, label = "LITTLE HAITI")+
#  annotate("text", x = 1300000.82, y = 6.85325579, label = "NAUTILUS")+
#  annotate("text", x = 631682.30, y = 3.57218432, label = "BISCAYNE POINT")+
#  annotate("text", x = 1557855.78, y = 2.81996319, label = "LA GORCE")+
#  annotate("text", x = 2031665.51, y = 1.95167248, label = "WYNWOOD - EDGEWATER")+
#  annotate("text", x = 890031.19, y = -2.16394539, label = "OVERTOWN")

```



### Does this model work equally for different demographic groups?
The variation in the MAPE of different neighborhoods suggests our model has limited generalizability. To test this, we look at how well it predicts prices across different racial and income contexts. In Figure 14.1 is the racial and income context of Miami. 

Table 5 and 6 confirm that this model applies slightly differently for different demographics. In Majority non-white neighborhoods, MAPE is 45% higher than in majority white neighborhoods. Similarly, MAPE is 41% higher in low income neighborhoods than in high income ones. 



```{r 14: race and income, fig.height=5, fig.width=8, message=FALSE, warning=FALSE}
#RACE & INCOME
  #make new layer
acsRaceIncome <-
  acs %>%
  mutate(raceContext= ifelse(pctWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(medHHInc > 49256.56, "High Income", "Low Income"))
#context
grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(acsRaceIncome), aes(fill = raceContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Race Context") +
    labs(title = "Figure 14.1: Race Context of Miami and Miami Beach") +
    theme_void() + theme(legend.position="bottom"),
  ggplot() + geom_sf(data = na.omit(acsRaceIncome), aes(fill = incomeContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
    labs(title = "Income Context of Miami and Miami Beach") +
    theme_void() + theme(legend.position="bottom"))

#tables
st_join(bothRegressions, acsRaceIncome) %>%
  filter(!is.na(raceContext)) %>%
  group_by(Regression, raceContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(raceContext, mean.MAPE) %>%
  pander(caption = "Table 5: MAPE by Neighborhood Racial Context")

st_join(bothRegressions, acsRaceIncome) %>%
  filter(!is.na(incomeContext)) %>%
  group_by(Regression, incomeContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  pander(caption = "Table 6: MAPE by Neighborhood Income Context")


```



## Discussion
In conclusion, our model is effective at predicting the distribution of home sale prices in the area as the pattern of predicted prices matches that of recent known home sale prices. However, it is limited in its ability to generalize across different types of neighborhoods. 

We have many interesting variables which had relationships with salePrice. As expected, distance from the shore correlates negatively with sale price, however the correlation was much weaker than expected because it only seemed to matter for the first mile from water. By contrast, floodInsureType correlated quite strongly with sale price. This feature comes from FEMA's rating of flood risk for different neighborhoods, and unlike our expectation, it was not only based on distance from the shoreline. 
  
In general, we found that houses' internal features were the strongest predictors of sale price. In addition, census features such as percent vacancy in a tract, median household income, and travel time to work correlated with sale price. Contrary to our expectations, distance to water, food establishments, businesses, school catchment areas, and park space are less correlated with home sale price. 

Our errors were higher than we would like. As discussed in the cross validation section, some houses had a mean average error of hundreds of thousands of dollars. Further, our errors were not distributed evenly, and were higher in low income and minority majority neighborhoods. Indeed, the highest MAPE values on our maps corresponded with the lowest income and lowest sale price neighborhoods. Based on our Moran's I test, however, we were successful at eliminating spatial clustering of errors. In all, our model predicted much better in high income, high observed sale price neighborhoods. This disparity is likely attributable to the fact that we used mainly positive home attributes and neighborhood amenities in our model. If we used more attributes such as poverty rate, race, and renter occupancy rate, our model may have been better at modeling majority-minority and low-income neighborhoods.


## Conclusion
To conclude, we do not believe that this model is ready for use by Zillow. Its errors are too significant, and its predictions are too uneven between different types of neighborhoods. Moving forward, we will add more features to this model to increase its ability to predict in neighborhoods with varying demographics. Additionally, we will fine tune features to account for non-linear correlations with sale price, such as by creating more categories.

